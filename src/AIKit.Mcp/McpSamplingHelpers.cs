// Generated by Copilot
using Microsoft.Extensions.AI;
using ModelContextProtocol;
using ModelContextProtocol.Protocol;
using ModelContextProtocol.Server;

namespace AIKit.Mcp;

/// <summary>
/// Provides simplified methods for handling sampling requests in MCP servers.
/// </summary>
public static class McpSamplingHelpers
{
    /// <summary>
    /// Creates a sampling handler that uses an IChatClient for LLM requests.
    /// </summary>
    /// <param name="chatClient">The chat client to use for sampling.</param>
    /// <returns>A sampling handler function.</returns>
    public static Func<CreateMessageRequestParams?, IProgress<ProgressNotificationValue>, CancellationToken, ValueTask<CreateMessageResult>> CreateSamplingHandler(
        IChatClient chatClient)
    {
        return chatClient.CreateSamplingHandler();
    }

    /// <summary>
    /// Performs a simple text generation request using the server's sampling capability.
    /// </summary>
    /// <param name="server">The MCP server instance.</param>
    /// <param name="prompt">The text prompt to send to the LLM.</param>
    /// <param name="maxTokens">Maximum number of tokens to generate.</param>
    /// <param name="temperature">Sampling temperature (0.0 to 1.0).</param>
    /// <param name="cancellationToken">The cancellation token.</param>
    /// <returns>The generated text content.</returns>
    public static async Task<string> GenerateTextAsync(
        McpServer server,
        string prompt,
        int maxTokens = 100,
        float? temperature = null,
        CancellationToken cancellationToken = default)
    {
        var result = await server.SampleAsync(new CreateMessageRequestParams
        {
            Messages = [new SamplingMessage
            {
                Role = Role.User,
                Content = [new TextContentBlock { Text = prompt }]
            }],
            MaxTokens = maxTokens,
            Temperature = temperature
        }, cancellationToken);

        return result.Content.OfType<TextContentBlock>().FirstOrDefault()?.Text ?? string.Empty;
    }

    /// <summary>
    /// Performs a chat-based sampling request using the server's sampling capability.
    /// </summary>
    /// <param name="server">The MCP server instance.</param>
    /// <param name="messages">The chat messages for the conversation.</param>
    /// <param name="maxTokens">Maximum number of tokens to generate.</param>
    /// <param name="temperature">Sampling temperature (0.0 to 1.0).</param>
    /// <param name="systemPrompt">Optional system prompt.</param>
    /// <param name="cancellationToken">The cancellation token.</param>
    /// <returns>The generated chat response.</returns>
    public static async Task<ChatResponse> GenerateChatResponseAsync(
        McpServer server,
        IEnumerable<ChatMessage> messages,
        int? maxTokens = null,
        float? temperature = null,
        string? systemPrompt = null,
        CancellationToken cancellationToken = default)
    {
        var chatOptions = new ChatOptions
        {
            MaxOutputTokens = maxTokens,
            Temperature = temperature,
            Instructions = systemPrompt
        };

        return await server.SampleAsync(messages, chatOptions, cancellationToken: cancellationToken);
    }

    /// <summary>
    /// Creates a task-based sampling request that can be polled for completion.
    /// </summary>
    /// <param name="server">The MCP server instance.</param>
    /// <param name="prompt">The text prompt to send to the LLM.</param>
    /// <param name="maxTokens">Maximum number of tokens to generate.</param>
    /// <param name="temperature">Sampling temperature (0.0 to 1.0).</param>
    /// <param name="taskMetadata">Metadata for the task including TTL.</param>
    /// <param name="cancellationToken">The cancellation token.</param>
    /// <returns>The MCP task that can be polled for results.</returns>
    public static async Task<McpTask> GenerateTextAsTaskAsync(
        McpServer server,
        string prompt,
        int maxTokens = 100,
        float? temperature = null,
        McpTaskMetadata? taskMetadata = null,
        CancellationToken cancellationToken = default)
    {
        taskMetadata ??= new McpTaskMetadata { TimeToLive = TimeSpan.FromMinutes(5) };

        return await server.SampleAsTaskAsync(new CreateMessageRequestParams
        {
            Messages = [new SamplingMessage
            {
                Role = Role.User,
                Content = [new TextContentBlock { Text = prompt }]
            }],
            MaxTokens = maxTokens,
            Temperature = temperature
        }, taskMetadata, cancellationToken);
    }
}